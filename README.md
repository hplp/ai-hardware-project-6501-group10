# AI_Hardware_Project

## Team Name: 
6501-group10

## Team Members:
- Melika Morsali
- Kavish Ranawella
- Hasantha Ekanayake

## Project Title:
Performance Evaluation of CNN and Attention Layers on Open-Source Platforms

## Project Description:
This project focuses on the implementation and performance evaluation of quantized CNN models on virtual hardware accelerator platforms.

## Key Objectives:
- Deploy Quantized Convolutional Neural Networks (CNNs) on Open Source Platforms
- Performance Analysis (latency, accuarcy, energy efficiency and resource utilization) 
- Extend this Approach to the Transformer Attention Layer

## Technology Stack:
(Python, PyTorch, NVDLA, ONNX)

## Expected Outcomes:

1. Quantization Effectiveness:
Post-training quantization (INT4, INT8) proves to be a practical technique for reducing model complexity while maintaining competitive accuracy, especially for CNNs and Transformer attention layers.
2. Virtual Accelerators as Viable Testing Platforms:
Platforms like NVDLA, VTA, and SCALE-Sim offer valuable insights into hardware-level performance, enabling efficient testing and deployment of quantized models without access to physical hardware.
3. Optimization Strategies for Diverse Architectures:
The project highlights the adaptability of quantization for different neural network architectures, emphasizing the need for architecture-specific optimizations to fully leverage hardware accelerators.
4. Informed Decision-Making for Edge Deployment:
The benchmarks and insights gained from this study provide a foundation for selecting the most suitable accelerator platform for deploying quantized models in real-world, resource-constrained environments.
5. Gaining Insights about Transformer Deployment:
Repurposing hardware architectures available for CNNs to be used to accelerate similar architectures in Transformers (like Attention).


## Timeline:
- Deploy Quantized Convolutional Neural Networks (CNNs) on Open Source Platforms - 1.5 weeks
- Performance Analysis (latency, accuarcy, energy efficiency and resource utilization) - 1.5 weeks
- Extend this Approach to the Transformer Attention Layer - 1 week 


[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/Buol6fpg)
[![Open in Codespaces](https://classroom.github.com/assets/launch-codespace-2972f46106e565e64193e422d61a12cf1da4916b45550586e14ef0a7c637dd04.svg)](https://classroom.github.com/open-in-codespaces?assignment_repo_id=16990746)
