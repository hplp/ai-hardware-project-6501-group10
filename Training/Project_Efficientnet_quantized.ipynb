{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiOHsMh8tOF0",
        "outputId": "a1c9d080-964f-4fa0-eb66-1a1ba113ede2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:08<00:00, 20.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 116MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training EfficientNet...\n",
            "Epoch [1], Step [0], Loss: 2.3858, Accuracy: 9.38%\n",
            "Epoch [1], Step [100], Loss: 2.2593, Accuracy: 17.30%\n",
            "Epoch [1], Step [200], Loss: 2.1315, Accuracy: 22.99%\n",
            "Epoch [1], Step [300], Loss: 2.0230, Accuracy: 27.57%\n",
            "Epoch [1], Step [400], Loss: 1.9319, Accuracy: 30.81%\n",
            "Epoch [1], Step [500], Loss: 1.8513, Accuracy: 33.83%\n",
            "Epoch [1], Step [600], Loss: 1.7904, Accuracy: 36.18%\n",
            "Epoch [1], Step [700], Loss: 1.7337, Accuracy: 38.25%\n",
            "Epoch [2], Step [0], Loss: 1.1864, Accuracy: 59.38%\n",
            "Epoch [2], Step [100], Loss: 1.3231, Accuracy: 53.12%\n",
            "Epoch [2], Step [200], Loss: 1.2922, Accuracy: 54.12%\n",
            "Epoch [2], Step [300], Loss: 1.2799, Accuracy: 54.34%\n",
            "Epoch [2], Step [400], Loss: 1.2613, Accuracy: 55.10%\n",
            "Epoch [2], Step [500], Loss: 1.2403, Accuracy: 55.94%\n",
            "Epoch [2], Step [600], Loss: 1.2286, Accuracy: 56.45%\n",
            "Epoch [2], Step [700], Loss: 1.2144, Accuracy: 56.93%\n",
            "Epoch [3], Step [0], Loss: 1.1619, Accuracy: 62.50%\n",
            "Epoch [3], Step [100], Loss: 1.1155, Accuracy: 61.23%\n",
            "Epoch [3], Step [200], Loss: 1.0848, Accuracy: 62.59%\n",
            "Epoch [3], Step [300], Loss: 1.0791, Accuracy: 62.65%\n",
            "Epoch [3], Step [400], Loss: 1.0680, Accuracy: 62.91%\n",
            "Epoch [3], Step [500], Loss: 1.0563, Accuracy: 63.22%\n",
            "Epoch [3], Step [600], Loss: 1.0500, Accuracy: 63.46%\n",
            "Epoch [3], Step [700], Loss: 1.0432, Accuracy: 63.70%\n",
            "Epoch [4], Step [0], Loss: 1.2823, Accuracy: 48.44%\n",
            "Epoch [4], Step [100], Loss: 0.9694, Accuracy: 65.95%\n",
            "Epoch [4], Step [200], Loss: 0.9685, Accuracy: 66.46%\n",
            "Epoch [4], Step [300], Loss: 0.9595, Accuracy: 66.76%\n",
            "Epoch [4], Step [400], Loss: 0.9528, Accuracy: 66.90%\n",
            "Epoch [4], Step [500], Loss: 0.9498, Accuracy: 67.17%\n",
            "Epoch [4], Step [600], Loss: 0.9413, Accuracy: 67.45%\n",
            "Epoch [4], Step [700], Loss: 0.9375, Accuracy: 67.51%\n",
            "Epoch [5], Step [0], Loss: 1.1624, Accuracy: 65.62%\n",
            "Epoch [5], Step [100], Loss: 0.8723, Accuracy: 69.59%\n",
            "Epoch [5], Step [200], Loss: 0.8816, Accuracy: 69.37%\n",
            "Epoch [5], Step [300], Loss: 0.8661, Accuracy: 69.95%\n",
            "Epoch [5], Step [400], Loss: 0.8668, Accuracy: 69.94%\n",
            "Epoch [5], Step [500], Loss: 0.8667, Accuracy: 69.97%\n",
            "Epoch [5], Step [600], Loss: 0.8660, Accuracy: 70.01%\n",
            "Epoch [5], Step [700], Loss: 0.8660, Accuracy: 70.08%\n",
            "Epoch [6], Step [0], Loss: 0.8268, Accuracy: 73.44%\n",
            "Epoch [6], Step [100], Loss: 0.8268, Accuracy: 71.32%\n",
            "Epoch [6], Step [200], Loss: 0.8204, Accuracy: 71.38%\n",
            "Epoch [6], Step [300], Loss: 0.8294, Accuracy: 71.04%\n",
            "Epoch [6], Step [400], Loss: 0.8261, Accuracy: 71.17%\n",
            "Epoch [6], Step [500], Loss: 0.8210, Accuracy: 71.44%\n",
            "Epoch [6], Step [600], Loss: 0.8174, Accuracy: 71.72%\n",
            "Epoch [6], Step [700], Loss: 0.8114, Accuracy: 71.87%\n",
            "Epoch [7], Step [0], Loss: 0.6984, Accuracy: 71.88%\n",
            "Epoch [7], Step [100], Loss: 0.7793, Accuracy: 72.57%\n",
            "Epoch [7], Step [200], Loss: 0.7670, Accuracy: 73.30%\n",
            "Epoch [7], Step [300], Loss: 0.7720, Accuracy: 73.27%\n",
            "Epoch [7], Step [400], Loss: 0.7787, Accuracy: 73.02%\n",
            "Epoch [7], Step [500], Loss: 0.7746, Accuracy: 73.11%\n",
            "Epoch [7], Step [600], Loss: 0.7708, Accuracy: 73.21%\n",
            "Epoch [7], Step [700], Loss: 0.7700, Accuracy: 73.22%\n",
            "Epoch [8], Step [0], Loss: 0.8205, Accuracy: 67.19%\n",
            "Epoch [8], Step [100], Loss: 0.7560, Accuracy: 74.20%\n",
            "Epoch [8], Step [200], Loss: 0.7395, Accuracy: 74.49%\n",
            "Epoch [8], Step [300], Loss: 0.7403, Accuracy: 74.42%\n",
            "Epoch [8], Step [400], Loss: 0.7379, Accuracy: 74.59%\n",
            "Epoch [8], Step [500], Loss: 0.7379, Accuracy: 74.61%\n",
            "Epoch [8], Step [600], Loss: 0.7355, Accuracy: 74.60%\n",
            "Epoch [8], Step [700], Loss: 0.7323, Accuracy: 74.75%\n",
            "Epoch [9], Step [0], Loss: 0.8266, Accuracy: 68.75%\n",
            "Epoch [9], Step [100], Loss: 0.7084, Accuracy: 75.87%\n",
            "Epoch [9], Step [200], Loss: 0.7127, Accuracy: 75.81%\n",
            "Epoch [9], Step [300], Loss: 0.7069, Accuracy: 75.80%\n",
            "Epoch [9], Step [400], Loss: 0.7060, Accuracy: 75.69%\n",
            "Epoch [9], Step [500], Loss: 0.7018, Accuracy: 75.84%\n",
            "Epoch [9], Step [600], Loss: 0.7019, Accuracy: 75.84%\n",
            "Epoch [9], Step [700], Loss: 0.7018, Accuracy: 75.71%\n",
            "Epoch [10], Step [0], Loss: 0.8745, Accuracy: 70.31%\n",
            "Epoch [10], Step [100], Loss: 0.6842, Accuracy: 76.31%\n",
            "Epoch [10], Step [200], Loss: 0.6825, Accuracy: 76.01%\n",
            "Epoch [10], Step [300], Loss: 0.6800, Accuracy: 76.25%\n",
            "Epoch [10], Step [400], Loss: 0.6815, Accuracy: 76.25%\n",
            "Epoch [10], Step [500], Loss: 0.6816, Accuracy: 76.19%\n",
            "Epoch [10], Step [600], Loss: 0.6777, Accuracy: 76.29%\n",
            "Epoch [10], Step [700], Loss: 0.6756, Accuracy: 76.37%\n",
            "Testing Full-Precision Model...\n",
            "Test Accuracy: 79.41%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data augmentation and normalization\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform_train, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform_test, download=True)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "# Load EfficientNet with pretrained weights\n",
        "model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "num_features = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(num_features, 10)\n",
        "model = model.to(device)\n",
        "\n",
        "# Train function\n",
        "def train(model, dataloader, cuda=False):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    model.train()\n",
        "    for epoch in range(10):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, (inputs, labels) in enumerate(dataloader):\n",
        "            if cuda:\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            if i % 100 == 0:\n",
        "                print(f'Epoch [{epoch + 1}], Step [{i}], Loss: {running_loss / (i + 1):.4f}, Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "# Evaluate the model\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "# Training the model\n",
        "print(\"Training EfficientNet...\")\n",
        "train(model, train_loader, cuda=(device.type == 'cuda'))\n",
        "\n",
        "# Testing the model\n",
        "print(\"Testing Full-Precision Model...\")\n",
        "full_precision_accuracy = test_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ni2I5NZzloc",
        "outputId": "93681c3a-2674-41c6-8715-3bc27e0561b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulating INT4 quantization using INT8.\n",
            "INT4 Quantized Model Accuracy: 79.39%\n",
            "INT8 Quantized Model Accuracy: 79.39%\n",
            "INT16 Quantized Model Accuracy: 79.41%\n",
            "\n",
            "Quantization Results:\n",
            "INT4 Accuracy: 79.39%\n",
            "INT8 Accuracy: 79.39%\n",
            "INT16 Accuracy: 79.41%\n"
          ]
        }
      ],
      "source": [
        "def quantize_and_test(model, test_loader, quant_type):\n",
        "\n",
        "    # Move the model to CPU for quantization\n",
        "    model.cpu()\n",
        "\n",
        "    if quant_type == 'int8':\n",
        "        quantized_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)  # INT8 quantization\n",
        "    elif quant_type == 'int16':\n",
        "        quantized_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.float16)  # INT16 simulation\n",
        "    elif quant_type == 'int4':\n",
        "        print(\"Simulating INT4 quantization using INT8.\")\n",
        "        quantized_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)  # Simulate INT4\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported quantization type: {quant_type}\")\n",
        "\n",
        "    # Test the quantized model\n",
        "    quantized_model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            # Move data to CPU for testing\n",
        "            images, labels = images.cpu(), labels.cpu()\n",
        "            outputs = quantized_model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"{quant_type.upper()} Quantized Model Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "# Quantized model evaluations\n",
        "quant_types = ['int4', 'int8', 'int16']\n",
        "quantized_accuracies = {}\n",
        "for qt in quant_types:\n",
        "    quantized_accuracies[qt] = quantize_and_test(model, test_loader, qt)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nQuantization Results:\")\n",
        "for qt, acc in quantized_accuracies.items():\n",
        "    print(f\"{qt.upper()} Accuracy: {acc:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}