{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQI8RcjA_0eo",
        "outputId": "d91a7334-611e-4677-fb2e-d1cc227680e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.25.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UpU9P3FL0R-",
        "outputId": "18fbf995-8a34-44cd-bf8a-9857ba681331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 496kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.34MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 11.3MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Size of FP32 Model (MB): 0.179057\n",
            "Epoch [1], Step [0], Loss: 2.3011, Accuracy: 10.94%\n",
            "Epoch [1], Step [100], Loss: 2.3004, Accuracy: 14.71%\n",
            "Epoch [1], Step [200], Loss: 2.2982, Accuracy: 18.31%\n",
            "Epoch [1], Step [300], Loss: 2.2952, Accuracy: 23.04%\n",
            "Epoch [1], Step [400], Loss: 2.2906, Accuracy: 28.16%\n",
            "Epoch [1], Step [500], Loss: 2.2825, Accuracy: 32.85%\n",
            "Epoch [1], Step [600], Loss: 2.2651, Accuracy: 35.34%\n",
            "Epoch [1], Step [700], Loss: 2.2093, Accuracy: 37.36%\n",
            "Epoch [1], Step [800], Loss: 2.0687, Accuracy: 41.68%\n",
            "Epoch [1], Step [900], Loss: 1.9100, Accuracy: 46.13%\n",
            "Epoch [2], Step [0], Loss: 0.6377, Accuracy: 78.12%\n",
            "Epoch [2], Step [100], Loss: 0.4674, Accuracy: 86.08%\n",
            "Epoch [2], Step [200], Loss: 0.4361, Accuracy: 86.82%\n",
            "Epoch [2], Step [300], Loss: 0.4047, Accuracy: 87.97%\n",
            "Epoch [2], Step [400], Loss: 0.3847, Accuracy: 88.52%\n",
            "Epoch [2], Step [500], Loss: 0.3710, Accuracy: 88.87%\n",
            "Epoch [2], Step [600], Loss: 0.3530, Accuracy: 89.44%\n",
            "Epoch [2], Step [700], Loss: 0.3392, Accuracy: 89.87%\n",
            "Epoch [2], Step [800], Loss: 0.3283, Accuracy: 90.23%\n",
            "Epoch [2], Step [900], Loss: 0.3186, Accuracy: 90.48%\n",
            "Epoch [3], Step [0], Loss: 0.2155, Accuracy: 95.31%\n",
            "Epoch [3], Step [100], Loss: 0.1944, Accuracy: 94.07%\n",
            "Epoch [3], Step [200], Loss: 0.1989, Accuracy: 93.96%\n",
            "Epoch [3], Step [300], Loss: 0.1946, Accuracy: 94.08%\n",
            "Epoch [3], Step [400], Loss: 0.1929, Accuracy: 94.17%\n",
            "Epoch [3], Step [500], Loss: 0.1883, Accuracy: 94.34%\n",
            "Epoch [3], Step [600], Loss: 0.1859, Accuracy: 94.40%\n",
            "Epoch [3], Step [700], Loss: 0.1832, Accuracy: 94.47%\n",
            "Epoch [3], Step [800], Loss: 0.1791, Accuracy: 94.58%\n",
            "Epoch [3], Step [900], Loss: 0.1760, Accuracy: 94.69%\n",
            "Epoch [4], Step [0], Loss: 0.1158, Accuracy: 95.31%\n",
            "Epoch [4], Step [100], Loss: 0.1462, Accuracy: 95.45%\n",
            "Epoch [4], Step [200], Loss: 0.1425, Accuracy: 95.58%\n",
            "Epoch [4], Step [300], Loss: 0.1403, Accuracy: 95.77%\n",
            "Epoch [4], Step [400], Loss: 0.1394, Accuracy: 95.79%\n",
            "Epoch [4], Step [500], Loss: 0.1391, Accuracy: 95.81%\n",
            "Epoch [4], Step [600], Loss: 0.1357, Accuracy: 95.86%\n",
            "Epoch [4], Step [700], Loss: 0.1339, Accuracy: 95.94%\n",
            "Epoch [4], Step [800], Loss: 0.1323, Accuracy: 95.98%\n",
            "Epoch [4], Step [900], Loss: 0.1323, Accuracy: 95.98%\n",
            "Epoch [5], Step [0], Loss: 0.1499, Accuracy: 95.31%\n",
            "Epoch [5], Step [100], Loss: 0.1179, Accuracy: 96.27%\n",
            "Epoch [5], Step [200], Loss: 0.1160, Accuracy: 96.48%\n",
            "Epoch [5], Step [300], Loss: 0.1140, Accuracy: 96.50%\n",
            "Epoch [5], Step [400], Loss: 0.1124, Accuracy: 96.55%\n",
            "Epoch [5], Step [500], Loss: 0.1127, Accuracy: 96.55%\n",
            "Epoch [5], Step [600], Loss: 0.1131, Accuracy: 96.58%\n",
            "Epoch [5], Step [700], Loss: 0.1127, Accuracy: 96.55%\n",
            "Epoch [5], Step [800], Loss: 0.1113, Accuracy: 96.53%\n",
            "Epoch [5], Step [900], Loss: 0.1109, Accuracy: 96.52%\n",
            "Epoch [6], Step [0], Loss: 0.1466, Accuracy: 95.31%\n",
            "Epoch [6], Step [100], Loss: 0.0971, Accuracy: 97.05%\n",
            "Epoch [6], Step [200], Loss: 0.1023, Accuracy: 96.94%\n",
            "Epoch [6], Step [300], Loss: 0.1005, Accuracy: 96.93%\n",
            "Epoch [6], Step [400], Loss: 0.0989, Accuracy: 97.02%\n",
            "Epoch [6], Step [500], Loss: 0.0971, Accuracy: 97.06%\n",
            "Epoch [6], Step [600], Loss: 0.0973, Accuracy: 97.08%\n",
            "Epoch [6], Step [700], Loss: 0.0969, Accuracy: 97.09%\n",
            "Epoch [6], Step [800], Loss: 0.0963, Accuracy: 97.09%\n",
            "Epoch [6], Step [900], Loss: 0.0957, Accuracy: 97.10%\n",
            "Epoch [7], Step [0], Loss: 0.0836, Accuracy: 96.88%\n",
            "Epoch [7], Step [100], Loss: 0.0900, Accuracy: 97.09%\n",
            "Epoch [7], Step [200], Loss: 0.0921, Accuracy: 97.12%\n",
            "Epoch [7], Step [300], Loss: 0.0897, Accuracy: 97.18%\n",
            "Epoch [7], Step [400], Loss: 0.0883, Accuracy: 97.23%\n",
            "Epoch [7], Step [500], Loss: 0.0880, Accuracy: 97.21%\n",
            "Epoch [7], Step [600], Loss: 0.0867, Accuracy: 97.23%\n",
            "Epoch [7], Step [700], Loss: 0.0861, Accuracy: 97.30%\n",
            "Epoch [7], Step [800], Loss: 0.0846, Accuracy: 97.36%\n",
            "Epoch [7], Step [900], Loss: 0.0848, Accuracy: 97.34%\n",
            "Epoch [8], Step [0], Loss: 0.1127, Accuracy: 96.88%\n",
            "Epoch [8], Step [100], Loss: 0.0841, Accuracy: 97.42%\n",
            "Epoch [8], Step [200], Loss: 0.0787, Accuracy: 97.49%\n",
            "Epoch [8], Step [300], Loss: 0.0791, Accuracy: 97.57%\n",
            "Epoch [8], Step [400], Loss: 0.0761, Accuracy: 97.67%\n",
            "Epoch [8], Step [500], Loss: 0.0756, Accuracy: 97.63%\n",
            "Epoch [8], Step [600], Loss: 0.0754, Accuracy: 97.63%\n",
            "Epoch [8], Step [700], Loss: 0.0764, Accuracy: 97.58%\n",
            "Epoch [8], Step [800], Loss: 0.0768, Accuracy: 97.57%\n",
            "Epoch [8], Step [900], Loss: 0.0758, Accuracy: 97.61%\n",
            "Epoch [9], Step [0], Loss: 0.0416, Accuracy: 98.44%\n",
            "Epoch [9], Step [100], Loss: 0.0646, Accuracy: 97.97%\n",
            "Epoch [9], Step [200], Loss: 0.0702, Accuracy: 97.85%\n",
            "Epoch [9], Step [300], Loss: 0.0703, Accuracy: 97.79%\n",
            "Epoch [9], Step [400], Loss: 0.0733, Accuracy: 97.67%\n",
            "Epoch [9], Step [500], Loss: 0.0725, Accuracy: 97.69%\n",
            "Epoch [9], Step [600], Loss: 0.0728, Accuracy: 97.69%\n",
            "Epoch [9], Step [700], Loss: 0.0729, Accuracy: 97.68%\n",
            "Epoch [9], Step [800], Loss: 0.0719, Accuracy: 97.71%\n",
            "Epoch [9], Step [900], Loss: 0.0711, Accuracy: 97.74%\n",
            "Epoch [10], Step [0], Loss: 0.0564, Accuracy: 98.44%\n",
            "Epoch [10], Step [100], Loss: 0.0588, Accuracy: 98.16%\n",
            "Epoch [10], Step [200], Loss: 0.0663, Accuracy: 98.02%\n",
            "Epoch [10], Step [300], Loss: 0.0675, Accuracy: 97.96%\n",
            "Epoch [10], Step [400], Loss: 0.0657, Accuracy: 97.98%\n",
            "Epoch [10], Step [500], Loss: 0.0664, Accuracy: 97.93%\n",
            "Epoch [10], Step [600], Loss: 0.0659, Accuracy: 97.94%\n",
            "Epoch [10], Step [700], Loss: 0.0662, Accuracy: 97.92%\n",
            "Epoch [10], Step [800], Loss: 0.0644, Accuracy: 97.99%\n",
            "Epoch [10], Step [900], Loss: 0.0658, Accuracy: 97.97%\n",
            "Finished Training\n",
            "Accuracy of the model: 98.16%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import onnx\n",
        "\n",
        "# Define the transformation\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load MNIST dataset\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Utility functions\n",
        "def print_size_of_model(model, name=\"Model\"):\n",
        "    \"\"\" Prints the real size of the model \"\"\"\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print(f'Size of {name} (MB): {os.path.getsize(\"temp.p\") / 1e6}')\n",
        "    os.remove('temp.p')\n",
        "\n",
        "def accuracy(output, target):\n",
        "    with torch.no_grad():\n",
        "        batch_size = target.size(0)\n",
        "        _, pred = output.topk(1, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "        return correct[:1].view(-1).float().sum(0, keepdim=True).mul_(100.0 / batch_size).item()\n",
        "\n",
        "# Define the LeNet model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, q=False):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5, bias=False)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5, bias=False)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(256, 120, bias=False)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84, bias=False)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, 10, bias=False)\n",
        "        self.q = q\n",
        "        if q:\n",
        "            self.quant = torch.quantization.QuantStub()\n",
        "            self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.q:\n",
        "            x = self.quant(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.fc3(x)\n",
        "        if self.q:\n",
        "            x = self.dequant(x)\n",
        "        return x\n",
        "\n",
        "# Train function\n",
        "def train(model, dataloader, cuda=False):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    model.train()\n",
        "    for epoch in range(10):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, data in enumerate(dataloader):\n",
        "            inputs, labels = data\n",
        "            if cuda:\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            if i % 100 == 0:\n",
        "                print(f'Epoch [{epoch + 1}], Step [{i}], Loss: {running_loss / (i + 1):.4f}, Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "# Test function\n",
        "def test(model, dataloader, cuda=False):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, labels = data\n",
        "            if cuda:\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model: {accuracy}%')\n",
        "    return accuracy\n",
        "\n",
        "# Train and test FP32 model\n",
        "net_fp32 = Net(q=False).cuda()\n",
        "print_size_of_model(net_fp32, \"FP32 Model\")\n",
        "train(net_fp32, trainloader, cuda=True)\n",
        "score_fp32 = test(net_fp32, testloader, cuda=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained FP32 model\n",
        "torch.save(net_fp32.state_dict(), \"lenet_int32.pth\")\n",
        "print(\"Trained model saved as lenet_int32.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu61oaFYHB6B",
        "outputId": "81d09104-af5e-491e-b341-5bd7e8f6101c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained model saved as lenet_int32.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Simulate INT4 quantization and dequantization\n",
        "def int4_quantize(tensor):\n",
        "    scale = 7 / tensor.abs().max()  # Scale factor to map to INT4 range\n",
        "    quantized_tensor = (tensor * scale).clamp(-7, 7).round().char()  # Quantize to INT4\n",
        "    return quantized_tensor, scale\n",
        "\n",
        "def int4_dequantize(quantized_tensor, scale):\n",
        "    return quantized_tensor.float() / scale  # Dequantize back to float\n",
        "\n",
        "# Simulate quantized forward pass for INT4 with input quantization\n",
        "def quantized_forward_int4_with_input_quant(model, x):\n",
        "    with torch.no_grad():\n",
        "        # Quantize the input\n",
        "        x_q, input_scale = int4_quantize(x)  # Quantize input and get scale\n",
        "\n",
        "        # Manually quantizing each layer's weights\n",
        "        conv1_w_q, conv1_scale = int4_quantize(model.conv1.weight.data)\n",
        "        conv2_w_q, conv2_scale = int4_quantize(model.conv2.weight.data)\n",
        "        fc1_w_q, fc1_scale = int4_quantize(model.fc1.weight.data)\n",
        "        fc2_w_q, fc2_scale = int4_quantize(model.fc2.weight.data)\n",
        "        fc3_w_q, fc3_scale = int4_quantize(model.fc3.weight.data)\n",
        "\n",
        "        # Forward pass with dequantized weights\n",
        "        x = F.conv2d(int4_dequantize(x_q, input_scale), int4_dequantize(conv1_w_q, conv1_scale), stride=1, padding=0)\n",
        "        x = model.relu1(x)\n",
        "        x = model.pool1(x)\n",
        "\n",
        "        x_q, input_scale = int4_quantize(x)  # Re-quantize the intermediate activation\n",
        "\n",
        "        x = F.conv2d(int4_dequantize(x_q, input_scale), int4_dequantize(conv2_w_q, conv2_scale), stride=1, padding=0)\n",
        "        x = model.relu2(x)\n",
        "        x = model.pool2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor correctly\n",
        "\n",
        "        # Dequantize and apply the fully connected layers\n",
        "        x_q, input_scale = int4_quantize(x)  # Quantize the flattened output before fully connected layers\n",
        "\n",
        "        x = F.linear(int4_dequantize(x_q, input_scale), int4_dequantize(fc1_w_q, fc1_scale))\n",
        "        x = model.relu3(x)\n",
        "\n",
        "        x_q, input_scale = int4_quantize(x)  # Re-quantize before the next layer\n",
        "\n",
        "        x = F.linear(int4_dequantize(x_q, input_scale), int4_dequantize(fc2_w_q, fc2_scale))\n",
        "        x = model.relu4(x)\n",
        "\n",
        "        x_q, input_scale = int4_quantize(x)  # Re-quantize before the final layer\n",
        "\n",
        "        x = F.linear(int4_dequantize(x_q, input_scale), int4_dequantize(fc3_w_q, fc3_scale))\n",
        "\n",
        "    return x\n",
        "\n",
        "# Quantized model testing function for INT4 with input quantization\n",
        "def test_quantized_int4_with_input_quant(model, dataloader, cuda=False):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, labels = data\n",
        "            if cuda:\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            outputs = quantized_forward_int4_with_input_quant(model, inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the quantized model (INT4) with input quantization on the test images: {accuracy}%')\n",
        "    return accuracy\n",
        "\n",
        "# Now test the INT4 quantized model\n",
        "print(\"Testing INT4 Quantized Model with Input Quantization...\")\n",
        "score_int4 = test_quantized_int4_with_input_quant(net_fp32, testloader, cuda=True)\n",
        "\n",
        "# Print model sizes and accuracies for comparison\n",
        "print(f'FP32 Model Accuracy: {score_fp32}%')\n",
        "print(f'INT4 Quantized Model Accuracy: {score_int4}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RQx7vwKNij3",
        "outputId": "c9c3e30b-5400-4001-cf40-8a4e6f780101"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing INT4 Quantized Model with Input Quantization...\n",
            "Accuracy of the quantized model (INT4) with input quantization on the test images: 96.51%\n",
            "FP32 Model Accuracy: 98.16%\n",
            "INT4 Quantized Model Accuracy: 96.51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the INT4 quantized model\n",
        "int4_model_state = {\n",
        "    \"conv1_weight\": int4_quantize(net_fp32.conv1.weight.data),\n",
        "    \"conv2_weight\": int4_quantize(net_fp32.conv2.weight.data),\n",
        "    \"fc1_weight\": int4_quantize(net_fp32.fc1.weight.data),\n",
        "    \"fc2_weight\": int4_quantize(net_fp32.fc2.weight.data),\n",
        "    \"fc3_weight\": int4_quantize(net_fp32.fc3.weight.data),\n",
        "}\n",
        "torch.save(int4_model_state, \"lenet_int4.pth\")\n",
        "print(\"Quantized INT4 model saved as lenet_int4.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pCH9Nt3HyUt",
        "outputId": "53b8e040-838f-4c1a-8572-4641019b3d3a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized INT4 model saved as lenet_int4.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INT16 quantization\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Simulate INT16 quantization and dequantization\n",
        "def int16_quantize(tensor):\n",
        "    scale = 32767 / tensor.abs().max()  # Scale factor to map to INT16 range\n",
        "    quantized_tensor = (tensor * scale).clamp(-32767, 32767).round().short()  # Quantize to INT16\n",
        "    return quantized_tensor, scale\n",
        "\n",
        "def int16_dequantize(quantized_tensor, scale):\n",
        "    return quantized_tensor.float() / scale  # Dequantize back to float\n",
        "\n",
        "# Simulate quantized forward pass for INT16 with input quantization\n",
        "def quantized_forward_int16_with_input_quant(model, x):\n",
        "    with torch.no_grad():\n",
        "        # Quantize the input\n",
        "        x_q, input_scale = int16_quantize(x)  # Quantize input and get scale\n",
        "\n",
        "        # Manually quantizing each layer's weights\n",
        "        conv1_w_q, conv1_scale = int16_quantize(model.conv1.weight.data)\n",
        "        conv2_w_q, conv2_scale = int16_quantize(model.conv2.weight.data)\n",
        "        fc1_w_q, fc1_scale = int16_quantize(model.fc1.weight.data)\n",
        "        fc2_w_q, fc2_scale = int16_quantize(model.fc2.weight.data)\n",
        "        fc3_w_q, fc3_scale = int16_quantize(model.fc3.weight.data)\n",
        "\n",
        "        # Forward pass with dequantized weights\n",
        "        x = F.conv2d(int16_dequantize(x_q, input_scale), int16_dequantize(conv1_w_q, conv1_scale), stride=1, padding=0)\n",
        "        x = model.relu1(x)\n",
        "        x = model.pool1(x)\n",
        "\n",
        "        x_q, input_scale = int16_quantize(x)  # Re-quantize the intermediate activation\n",
        "\n",
        "        x = F.conv2d(int16_dequantize(x_q, input_scale), int16_dequantize(conv2_w_q, conv2_scale), stride=1, padding=0)\n",
        "        x = model.relu2(x)\n",
        "        x = model.pool2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor correctly\n",
        "\n",
        "        # Dequantize and apply the fully connected layers\n",
        "        x_q, input_scale = int16_quantize(x)  # Quantize the flattened output before fully connected layers\n",
        "\n",
        "        x = F.linear(int16_dequantize(x_q, input_scale), int16_dequantize(fc1_w_q, fc1_scale))\n",
        "        x = model.relu3(x)\n",
        "\n",
        "        x_q, input_scale = int16_quantize(x)  # Re-quantize before the next layer\n",
        "\n",
        "        x = F.linear(int16_dequantize(x_q, input_scale), int16_dequantize(fc2_w_q, fc2_scale))\n",
        "        x = model.relu4(x)\n",
        "\n",
        "        x_q, input_scale = int16_quantize(x)  # Re-quantize before the final layer\n",
        "\n",
        "        x = F.linear(int16_dequantize(x_q, input_scale), int16_dequantize(fc3_w_q, fc3_scale))\n",
        "\n",
        "    return x\n",
        "\n",
        "# Quantized model testing function for INT16 with input quantization\n",
        "def test_quantized_int16_with_input_quant(model, dataloader, cuda=False):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, labels = data\n",
        "            if cuda:\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            outputs = quantized_forward_int16_with_input_quant(model, inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the quantized model (INT16) with input quantization on the test images: {accuracy}%')\n",
        "    return accuracy\n",
        "\n",
        "# Now test the INT16 quantized model\n",
        "print(\"Testing INT16 Quantized Model with Input Quantization...\")\n",
        "score_int16 = test_quantized_int16_with_input_quant(net_fp32, testloader, cuda=True)\n",
        "\n",
        "# Print model sizes and accuracies for comparison\n",
        "print(f'FP32 Model Accuracy: {score_fp32}%')\n",
        "print(f'INT16 Quantized Model Accuracy: {score_int16}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_5dJaE6OGlm",
        "outputId": "3fc62699-7694-4a65-ed3f-c422a85a3024"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing INT16 Quantized Model with Input Quantization...\n",
            "Accuracy of the quantized model (INT16) with input quantization on the test images: 98.16%\n",
            "FP32 Model Accuracy: 98.16%\n",
            "INT16 Quantized Model Accuracy: 98.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the INT16 quantized model\n",
        "int16_model_state = {\n",
        "    \"conv1_weight\": int16_quantize(net_fp32.conv1.weight.data),\n",
        "    \"conv2_weight\": int16_quantize(net_fp32.conv2.weight.data),\n",
        "    \"fc1_weight\": int16_quantize(net_fp32.fc1.weight.data),\n",
        "    \"fc2_weight\": int16_quantize(net_fp32.fc2.weight.data),\n",
        "    \"fc3_weight\": int16_quantize(net_fp32.fc3.weight.data),\n",
        "}\n",
        "torch.save(int16_model_state, \"lenet_int16.pth\")\n",
        "print(\"Quantized INT16 model saved as lenet_int16.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1FkIhNZIOoR",
        "outputId": "67e8655b-a416-4aca-c4f9-be7a33fe8919"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized INT16 model saved as lenet_int16.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INT8:\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Simulate INT8 quantization and dequantization\n",
        "def int8_quantize(tensor):\n",
        "    scale = 127 / tensor.abs().max()  # Scale factor to map to INT8 range\n",
        "    quantized_tensor = (tensor * scale).clamp(-127, 127).round().char()  # Quantize to INT8\n",
        "    return quantized_tensor, scale\n",
        "\n",
        "def int8_dequantize(quantized_tensor, scale):\n",
        "    return quantized_tensor.float() / scale  # Dequantize back to float\n",
        "\n",
        "# Simulate quantized forward pass for INT8 with input quantization\n",
        "def quantized_forward_int8_with_input_quant(model, x):\n",
        "    with torch.no_grad():\n",
        "        # Quantize the input\n",
        "        x_q, input_scale = int8_quantize(x)  # Quantize input and get scale\n",
        "\n",
        "        # Manually quantizing each layer's weights\n",
        "        conv1_w_q, conv1_scale = int8_quantize(model.conv1.weight.data)\n",
        "        conv2_w_q, conv2_scale = int8_quantize(model.conv2.weight.data)\n",
        "        fc1_w_q, fc1_scale = int8_quantize(model.fc1.weight.data)\n",
        "        fc2_w_q, fc2_scale = int8_quantize(model.fc2.weight.data)\n",
        "        fc3_w_q, fc3_scale = int8_quantize(model.fc3.weight.data)\n",
        "\n",
        "        # Forward pass with dequantized weights\n",
        "        x = F.conv2d(int8_dequantize(x_q, input_scale), int8_dequantize(conv1_w_q, conv1_scale), stride=1, padding=0)\n",
        "        x = model.relu1(x)\n",
        "        x = model.pool1(x)\n",
        "\n",
        "        x_q, input_scale = int8_quantize(x)  # Re-quantize the intermediate activation\n",
        "\n",
        "        x = F.conv2d(int8_dequantize(x_q, input_scale), int8_dequantize(conv2_w_q, conv2_scale), stride=1, padding=0)\n",
        "        x = model.relu2(x)\n",
        "        x = model.pool2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor correctly\n",
        "\n",
        "        # Dequantize and apply the fully connected layers\n",
        "        x_q, input_scale = int8_quantize(x)  # Quantize the flattened output before fully connected layers\n",
        "\n",
        "        x = F.linear(int8_dequantize(x_q, input_scale), int8_dequantize(fc1_w_q, fc1_scale))\n",
        "        x = model.relu3(x)\n",
        "\n",
        "        x_q, input_scale = int8_quantize(x)  # Re-quantize before the next layer\n",
        "\n",
        "        x = F.linear(int8_dequantize(x_q, input_scale), int8_dequantize(fc2_w_q, fc2_scale))\n",
        "        x = model.relu4(x)\n",
        "\n",
        "        x_q, input_scale = int8_quantize(x)  # Re-quantize before the final layer\n",
        "\n",
        "        x = F.linear(int8_dequantize(x_q, input_scale), int8_dequantize(fc3_w_q, fc3_scale))\n",
        "\n",
        "    return x\n",
        "\n",
        "# Quantized model testing function for INT8 with input quantization\n",
        "def test_quantized_int8_with_input_quant(model, dataloader, cuda=False):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, labels = data\n",
        "            if cuda:\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            outputs = quantized_forward_int8_with_input_quant(model, inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the quantized model (INT8) with input quantization on the test images: {accuracy}%')\n",
        "    return accuracy\n",
        "\n",
        "# Now test the INT8 quantized model\n",
        "print(\"Testing INT8 Quantized Model with Input Quantization...\")\n",
        "score_int8 = test_quantized_int8_with_input_quant(net_fp32, testloader, cuda=True)\n",
        "\n",
        "# Print model sizes and accuracies for comparison\n",
        "print(f'FP32 Model Accuracy: {score_fp32}%')\n",
        "print(f'INT8 Quantized Model Accuracy: {score_int8}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lcnJm-ZOPcA",
        "outputId": "224d8232-7776-4f05-e62f-7595216edc7c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing INT8 Quantized Model with Input Quantization...\n",
            "Accuracy of the quantized model (INT8) with input quantization on the test images: 98.06%\n",
            "FP32 Model Accuracy: 98.16%\n",
            "INT8 Quantized Model Accuracy: 98.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the INT16 quantized model\n",
        "int8_model_state = {\n",
        "    \"conv1_weight\": int8_quantize(net_fp32.conv1.weight.data),\n",
        "    \"conv2_weight\": int8_quantize(net_fp32.conv2.weight.data),\n",
        "    \"fc1_weight\": int8_quantize(net_fp32.fc1.weight.data),\n",
        "    \"fc2_weight\": int8_quantize(net_fp32.fc2.weight.data),\n",
        "    \"fc3_weight\": int8_quantize(net_fp32.fc3.weight.data),\n",
        "}\n",
        "torch.save(int8_model_state, \"lenet_int8.pth\")\n",
        "print(\"Quantized INT8 model saved as lenet_int8.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7rsAx4BIsAK",
        "outputId": "e7c88f56-1dd7-48d7-c8e1-5fd1dca6ced7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized INT8 model saved as lenet_int8.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model sizes and accuracies for comparison\n",
        "print(f'FP32 Model Accuracy: {score_fp32}%')\n",
        "print(f'INT16 Quantized Model Accuracy: {score_int16}%')\n",
        "print(f'INT8 Quantized Model Accuracy: {score_int8}%')\n",
        "print(f'INT4 Quantized Model Accuracy: {score_int4}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTcpEl0COSqu",
        "outputId": "220cf4c2-246f-4804-c6f8-f50751406e9b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP32 Model Accuracy: 98.16%\n",
            "INT16 Quantized Model Accuracy: 98.16%\n",
            "INT8 Quantized Model Accuracy: 98.06%\n",
            "INT4 Quantized Model Accuracy: 96.51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to ONNX\n",
        "def convert_to_onnx(model, model_name):\n",
        "    dummy_input = torch.randn(1, 1, 28, 28).cuda()  # Example input tensor for MNIST\n",
        "    onnx_path = f\"{model_name}.onnx\"\n",
        "    torch.onnx.export(model, dummy_input, onnx_path, export_params=True, opset_version=10,\n",
        "                      do_constant_folding=True, input_names=['input'], output_names=['output'])\n",
        "    print(f\"Model has been converted to ONNX and saved at {onnx_path}\")\n",
        "\n",
        "# Convert FP32 model to ONNX\n",
        "convert_to_onnx(net_fp32, \"lenet_fp32\")\n",
        "\n",
        "# Load quantized models and convert to ONNX\n",
        "quantized_model_paths = {\n",
        "    \"lenet_int4\": \"./lenet_int4.pth\",\n",
        "    \"lenet_int8\": \"./lenet_int8.pth\",\n",
        "    \"lenet_int16\": \"./lenet_int16.pth\"\n",
        "}\n",
        "\n",
        "for model_name, model_path in quantized_model_paths.items():\n",
        "    model = Net(q=False).cuda()\n",
        "    state_dict = torch.load(model_path, weights_only=True)\n",
        "    # Update state_dict keys to match model keys\n",
        "    updated_state_dict = {k.replace('_weight', '.weight'): v[0] if isinstance(v, tuple) else v for k, v in state_dict.items()}\n",
        "    model.load_state_dict(updated_state_dict)\n",
        "    convert_to_onnx(model, model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqNv0dGk_c9P",
        "outputId": "1095c096-517e-485f-89c9-965b00d43d70"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has been converted to ONNX and saved at lenet_fp32.onnx\n",
            "Model has been converted to ONNX and saved at lenet_int4.onnx\n",
            "Model has been converted to ONNX and saved at lenet_int8.onnx\n",
            "Model has been converted to ONNX and saved at lenet_int16.onnx\n"
          ]
        }
      ]
    }
  ]
}
