{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7613ed3a-2aad-446d-a57e-50c5bde30e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 186242737.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 14373005.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 73548275.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 17606773.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4617, Accuracy: 84.07%\n",
      "Epoch [2/10], Loss: 0.0840, Accuracy: 97.52%\n",
      "Epoch [3/10], Loss: 0.0666, Accuracy: 98.05%\n",
      "Epoch [4/10], Loss: 0.0612, Accuracy: 98.19%\n",
      "Epoch [5/10], Loss: 0.0547, Accuracy: 98.39%\n",
      "Epoch [6/10], Loss: 0.0519, Accuracy: 98.47%\n",
      "Epoch [7/10], Loss: 0.0499, Accuracy: 98.50%\n",
      "Epoch [8/10], Loss: 0.0453, Accuracy: 98.64%\n",
      "Epoch [9/10], Loss: 0.0424, Accuracy: 98.82%\n",
      "Epoch [10/10], Loss: 0.0425, Accuracy: 98.78%\n",
      "FP32 Model Trained and Saved.\n",
      "Accuracy: 99.19%\n",
      "FP32 Model Accuracy on MNIST: 99.19%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define AlexNet Model for MNIST\n",
    "class AlexNetMNIST(nn.Module):\n",
    "    def __init__(self, q=False):\n",
    "        super(AlexNetMNIST, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=2),  # Conv1\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),  # Conv2\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),  # Conv3\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),  # Conv4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),  # Conv5\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),  # FC6\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),  # FC7\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 10),  # FC8 (output for MNIST)\n",
    "        )\n",
    "        self.q = q\n",
    "        if q:\n",
    "            self.quant = torch.quantization.QuantStub()\n",
    "            self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.q:\n",
    "            x = self.quant(x)\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.classifier(x)\n",
    "        if self.q:\n",
    "            x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# MNIST Dataset Preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize for AlexNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize for grayscale images\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "# Training Function\n",
    "def train(model, dataloader, epochs=10, cuda=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs, labels = data\n",
    "            if cuda:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(dataloader):.4f}, Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "# Testing Function\n",
    "def test(model, dataloader, cuda=False):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            if cuda:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy}%')\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Train the FP32 Model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "alexnet_fp32 = AlexNetMNIST(q=False).to(device)\n",
    "train(alexnet_fp32, trainloader, epochs=10, cuda=(device == 'cuda'))\n",
    "torch.save(alexnet_fp32.state_dict(), \"alexnet_fp32_mnist.pth\")\n",
    "print(\"FP32 Model Trained and Saved.\")\n",
    "\n",
    "# Test FP32 Model\n",
    "fp32_accuracy = test(alexnet_fp32, testloader, cuda=(device == 'cuda'))\n",
    "print(f\"FP32 Model Accuracy on MNIST: {fp32_accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb352287-453d-4959-9e6d-506779d9b110",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Per-Layer INT8 Quantized Model...\n",
      "Quantized Model Accuracy: 98.6%\n",
      "INT8 Quantized Model Accuracy: 98.6%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Per-Layer Quantization\n",
    "def per_layer_quantize(tensor):\n",
    "    max_val = tensor.abs().amax()\n",
    "    scale = 127 / max_val\n",
    "    quantized_tensor = (tensor * scale).clamp(-127, 127).round().char()\n",
    "    return quantized_tensor, scale\n",
    "\n",
    "def per_layer_dequantize(quantized_tensor, scale):\n",
    "    return quantized_tensor.float() / scale\n",
    "\n",
    "# Quantized Forward Pass\n",
    "def quantized_forward_per_layer(model, x, quantize_fn, dequantize_fn):\n",
    "    with torch.no_grad():\n",
    "        weights_q = {}\n",
    "        scales = {}\n",
    "\n",
    "        # Quantize weights\n",
    "        for name, param in model.named_parameters():\n",
    "            weights_q[name], scales[name] = quantize_fn(param.data)\n",
    "\n",
    "        # Forward pass through features\n",
    "        for i, layer in enumerate(model.features):\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                key = f\"features.{i}.weight\"\n",
    "                activation_scale = 127 / (x.abs().amax() + 1e-8)\n",
    "                x = F.conv2d(\n",
    "                    x / activation_scale,\n",
    "                    dequantize_fn(weights_q[key], scales[key]),\n",
    "                    stride=layer.stride,\n",
    "                    padding=layer.padding\n",
    "                )\n",
    "                x = (x * activation_scale).clamp(-127, 127).round().char()\n",
    "            elif isinstance(layer, nn.MaxPool2d):\n",
    "                x = x.float()  # Convert back to Float for pooling\n",
    "                x = layer(x)\n",
    "                activation_scale = 127 / (x.abs().amax() + 1e-8)  # Recompute scale\n",
    "                x = (x * activation_scale).clamp(-127, 127).round().char()  # Requantize\n",
    "            elif isinstance(layer, nn.ReLU):\n",
    "                x = x.float()  # Convert back to Float for ReLU\n",
    "                x = layer(x)\n",
    "                activation_scale = 127 / (x.abs().amax() + 1e-8)  # Recompute scale\n",
    "                x = (x * activation_scale).clamp(-127, 127).round().char()  # Requantize\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Forward pass through classifier\n",
    "        for i, layer in enumerate(model.classifier):\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                key = f\"classifier.{i}.weight\"\n",
    "                activation_scale = 127 / (x.abs().amax() + 1e-8)\n",
    "                x = F.linear(\n",
    "                    x / activation_scale,\n",
    "                    dequantize_fn(weights_q[key], scales[key])\n",
    "                )\n",
    "                x = (x * activation_scale).clamp(-127, 127).round().char()\n",
    "            elif isinstance(layer, nn.ReLU) or isinstance(layer, nn.Dropout):\n",
    "                x = x.float()  # Convert back to Float for ReLU or Dropout\n",
    "                x = layer(x)\n",
    "                activation_scale = 127 / (x.abs().amax() + 1e-8)  # Recompute scale\n",
    "                x = (x * activation_scale).clamp(-127, 127).round().char()  # Requantize\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Quantized Model Testing\n",
    "def test_quantized_per_layer(model, dataloader, device, quantize_fn, dequantize_fn):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = quantized_forward_per_layer(model, inputs, quantize_fn, dequantize_fn)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Quantized Model Accuracy: {accuracy}%\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Quantized Testing for CIFAR-10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "alexnet_fp32.load_state_dict(torch.load(\"alexnet_fp32_mnist.pth\"))\n",
    "print(\"Testing Per-Layer INT8 Quantized Model...\")\n",
    "int8_accuracy = test_quantized_per_layer(alexnet_fp32, testloader, device, per_layer_quantize, per_layer_dequantize)\n",
    "print(f\"INT8 Quantized Model Accuracy: {int8_accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6db61ca2-5ac9-453f-a003-64cb0f1e3961",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing INT16 Quantized Model...\n",
      "Quantized Model Accuracy: 98.64%\n",
      "INT16 Quantized Model Accuracy: 98.64%\n"
     ]
    }
   ],
   "source": [
    "# INT16 Per-Layer Quantization\n",
    "def per_layer_quantize_int16(tensor):\n",
    "    max_val = tensor.abs().amax()\n",
    "    scale = 32767 / max_val\n",
    "    quantized_tensor = (tensor * scale).clamp(-32767, 32767).round().short()\n",
    "    return quantized_tensor, scale\n",
    "\n",
    "def per_layer_dequantize_int16(quantized_tensor, scale):\n",
    "    return quantized_tensor.float() / scale\n",
    "\n",
    "# INT16 Quantized Forward Pass\n",
    "def quantized_forward_per_layer_int16(model, x, quantize_fn, dequantize_fn):\n",
    "    with torch.no_grad():\n",
    "        weights_q = {}\n",
    "        scales = {}\n",
    "\n",
    "        # Quantize weights\n",
    "        for name, param in model.named_parameters():\n",
    "            weights_q[name], scales[name] = quantize_fn(param.data)\n",
    "\n",
    "        # Forward pass through features\n",
    "        for i, layer in enumerate(model.features):\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                key = f\"features.{i}.weight\"\n",
    "                activation_scale = 32767 / (x.abs().amax() + 1e-8)\n",
    "                x = F.conv2d(\n",
    "                    x / activation_scale,\n",
    "                    dequantize_fn(weights_q[key], scales[key]),\n",
    "                    stride=layer.stride,\n",
    "                    padding=layer.padding\n",
    "                )\n",
    "                x = (x * activation_scale).clamp(-32767, 32767).round().short()\n",
    "            elif isinstance(layer, nn.MaxPool2d):\n",
    "                x = x.float()  # Convert back to Float for pooling\n",
    "                x = layer(x)\n",
    "                activation_scale = 32767 / (x.abs().amax() + 1e-8)  # Recompute scale\n",
    "                x = (x * activation_scale).clamp(-32767, 32767).round().short()  # Requantize\n",
    "            elif isinstance(layer, nn.ReLU):\n",
    "                x = x.float()  # Convert back to Float for ReLU\n",
    "                x = layer(x)\n",
    "                activation_scale = 32767 / (x.abs().amax() + 1e-8)  # Recompute scale\n",
    "                x = (x * activation_scale).clamp(-32767, 32767).round().short()  # Requantize\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Forward pass through classifier\n",
    "        for i, layer in enumerate(model.classifier):\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                key = f\"classifier.{i}.weight\"\n",
    "                activation_scale = 32767 / (x.abs().amax() + 1e-8)\n",
    "                x = F.linear(\n",
    "                    x / activation_scale,\n",
    "                    dequantize_fn(weights_q[key], scales[key])\n",
    "                )\n",
    "                x = (x * activation_scale).clamp(-32767, 32767).round().short()\n",
    "            elif isinstance(layer, nn.ReLU) or isinstance(layer, nn.Dropout):\n",
    "                x = x.float()  # Convert back to Float for ReLU or Dropout\n",
    "                x = layer(x)\n",
    "                activation_scale = 32767 / (x.abs().amax() + 1e-8)  # Recompute scale\n",
    "                x = (x * activation_scale).clamp(-32767, 32767).round().short()  # Requantize\n",
    "\n",
    "        return x\n",
    "\n",
    "# Quantized Model Testing for INT16\n",
    "def test_quantized_per_layer_int16(model, dataloader, device, quantize_fn, dequantize_fn):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = quantized_forward_per_layer_int16(model, inputs, quantize_fn, dequantize_fn)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Quantized Model Accuracy: {accuracy}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Test INT16 Quantized Model\n",
    "print(\"Testing INT16 Quantized Model...\")\n",
    "int16_accuracy = test_quantized_per_layer_int16(\n",
    "    alexnet_fp32, testloader, device, per_layer_quantize_int16, per_layer_dequantize_int16\n",
    ")\n",
    "print(f\"INT16 Quantized Model Accuracy: {int16_accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2df63a0b-35a3-4dd2-8360-25f0c6c5a73e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 Model exported to alexnet_fp32_mnist.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx\n",
    "\n",
    "# Convert FP32 Model to ONNX\n",
    "def convert_fp32_to_onnx(model, onnx_filename, input_size=(1, 1, 224, 224)):\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(*input_size).to(next(model.parameters()).device)\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_filename,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        input_names=['input'],\n",
    "        output_names=['output']\n",
    "    )\n",
    "    print(f\"FP32 Model exported to {onnx_filename}\")\n",
    "\n",
    "# Example Usage\n",
    "convert_fp32_to_onnx(alexnet_fp32, \"alexnet_fp32_mnist.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b66f4-5e58-4eb6-a41e-8730d371cdb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.4.0",
   "language": "python",
   "name": "pytorch-2.4.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
